# DeepSeek AI-Based Automated Script Generation
​​​​
## Overview

This project integrates the **DeepSeek AI model** with **FastAPI** to provide an automated script generation and operations solution. Users can generate scripts (e.g., shell, Python, SQL) based on their specific requirements. The system also includes a download feature for generated scripts, implemented using **HTML**, **CSS**, and **JavaScript**, with **highlight.js** for syntax highlighting. FastAPI, paired with a template engine, ensures high-speed responses and a smooth user experience.

## Features

- **Script Generation**: Automatically generates various types of scripts (Python, Shell, SQL, etc.) based on user input.
- **Downloadable Scripts**: Users can download generated scripts in their respective formats (e.g., `.py`, `.sh`).
- **Code Syntax Highlighting**: The system uses **highlight.js** for syntax highlighting, making the generated code easy to read.
- **FastAPI for Speed**: The backend is built with **FastAPI** to ensure fast response times and high concurrency.
- **Fine-Tuning Capability**: Easily integrate custom datasets to fine-tune the model and improve script generation quality for specific tasks.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/louis-VI/AIDevops_DeepSeek.git
   cd AIDevops_DeepSeek
   ```

2. Install the necessary dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up the **DeepSeek API** to generate scripts by obtaining your API key and configuring the system to use it.

4. Run the FastAPI server:
   ```bash
   uvicorn main:app --reload
   ```

## Usage

- **Generate Scripts**: Use the provided HTML interface to input your requirements and generate scripts. The system will interact with the DeepSeek API to generate the script, and you can download it in the appropriate format.
- **Code Syntax Highlighting**: Generated scripts are displayed with syntax highlighting using **highlight.js** to make them easier to read.
- **FastAPI Backend**: The backend, built with **FastAPI**, ensures rapid script generation, handling multiple requests concurrently and efficiently.

### Example

A sample Python script generated by the system to scrape a webpage and save data to CSV:

```python
import requests
from bs4 import BeautifulSoup
import csv

url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

data = []
for item in soup.find_all('div', class_='product'):
    name = item.find('h2').text
    price = item.find('span', class_='price').text
    data.append([name, price])

with open('products.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Name', 'Price'])
    writer.writerows(data)

print("Data saved to products.csv")
```

## Fine-Tuning the Model

### 1. Prepare Your Custom Dataset

To fine-tune the model for generating domain-specific scripts, prepare a dataset that contains pairs of user queries and corresponding scripts.

Example format:

```json
[
    {
        "input": "Generate a Python script to scrape a website.",
        "output": "import requests\nfrom bs4 import BeautifulSoup\n..."
    },
    {
        "input": "Create a shell script to monitor CPU usage.",
        "output": "#!/bin/bash\ncpu_usage=$(top -bn1 | grep 'Cpu(s)' | awk '{print $2 + $4}')"
    }
]
```

### 2. Fine-Tuning Using Hugging Face Transformers

Fine-tuning can be done using the `transformers` library. Below is an example of how to fine-tune the model with your custom dataset.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

# Load pre-trained DeepSeek model
model_name = "deepseek-ai/deepseek-llm-7b-chat"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Prepare your dataset (you'll need to load and preprocess your custom dataset)
train_dataset = ...
val_dataset = ...

# Fine-tuning configuration
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=2,
    num_train_epochs=3
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Start fine-tuning
trainer.train()
```

### 3. Using the Fine-Tuned Model

After fine-tuning, you can use the updated model to generate scripts based on your specific domain:

```python
model = AutoModelForCausalLM.from_pretrained("./results")
tokenizer = AutoTokenizer.from_pretrained(model_name)

input_text = "Generate a script to monitor disk usage."
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(inputs["input_ids"])

generated_script = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_script)
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Key Changes:
- **FastAPI Integration**: Emphasized that the backend is built with **FastAPI** to ensure fast responses and high concurrency, which is ideal for handling multiple requests in real-time.
- **Template Engine**: Mentioned the use of a template engine in conjunction with FastAPI to improve rendering of dynamic HTML content.
- **Fine-Tuning**: Retained the fine-tuning section with relevant steps to add custom datasets for domain-specific model improvements.
